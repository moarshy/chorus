# Sprint 9: Context Usage Indicator

## Overview

This feature adds visibility into Claude's context window consumption during conversations. Users currently lack insight into how much of the context window has been used, making it difficult to gauge when a conversation might need to be restarted or when context compaction might occur.

## Problem Statement

- Users have no visibility into context window consumption
- No way to determine remaining capacity before hitting limits
- Difficult to decide when to start a new conversation
- No warning when approaching context limits

## References

- **GitHub Issue**: [NapthaAI/chorus#7](https://github.com/NapthaAI/chorus/issues/7) - Add context usage percentage indicator
- **Anthropic SDK Issue**: [anthropics/claude-agent-sdk-typescript#66](https://github.com/anthropics/claude-agent-sdk-typescript/issues/66) - Context window usage calculation (unresolved)

## Technical Constraints

### SDK Limitations

The Claude Agent SDK does **not** provide:
- Real-time context window utilization percentage
- Breakdown by component (system prompt, tools, conversation history)
- Remaining capacity prediction
- Post-compaction context size
- Equivalent to CLI's `/context` command

### What the SDK Provides

Per assistant message via `usage` field:
- `input_tokens` - Tokens in the input (prompt + context)
- `output_tokens` - Tokens generated by assistant
- `cache_creation_input_tokens` - Tokens written to prompt cache
- `cache_read_input_tokens` - Tokens read from prompt cache (NOT counted against context)

### Calculation Approach

Since there's no official guidance from Anthropic (see SDK issue #66), we use a **best-effort estimation**:

```
Context Usage % = (cumulative_input_tokens / model_context_limit) × 100
```

**Important Notes:**
- `input_tokens` represents tokens sent to the model per turn
- This is cumulative across turns but the model manages context internally
- Cache read tokens are stored externally and don't count against context
- The SDK performs automatic compaction when context fills up
- Our estimate may diverge from actual usage due to compaction

### Model Context Limits

| Model | Context Window |
|-------|---------------|
| Default (Sonnet 4.5) | 200,000 tokens |
| Opus 4.5 | 200,000 tokens |
| Sonnet 4.5 (1M) | 1,000,000 tokens |
| Haiku 4.5 | 200,000 tokens |

## User Stories

### US-1: Context Badge in Chat Header

**As a** user chatting with an agent
**I want to** see context usage at a glance in the chat header
**So that** I can monitor usage without opening the details panel

**Acceptance Criteria:**
- Compact badge showing "Context: ~23%" in ConversationToolbar
- Tilde (~) indicates this is an estimate
- Color-coded indicators:
  - Green: 0-50%
  - Yellow: 50-75%
  - Orange: 75-90%
  - Red: 90%+
- Warning icon appears at 75%+
- Tooltip on hover shows: "~45,234 / 200,000 tokens (estimated)"
- Updates after each assistant response

### US-2: Enhanced Context Section in Details Panel

**As a** user monitoring conversation metrics
**I want to** see detailed token breakdown with percentage
**So that** I understand the full context consumption picture

**Acceptance Criteria:**
- Context section shows percentage alongside existing metrics
- Display breakdown:
  - Input tokens (with percentage of limit)
  - Output tokens
  - Cache read tokens (with note: "not counted against context")
  - Cache creation tokens
  - Total cost
- Visual progress bar showing estimated context usage
- Clear disclaimer: "Estimated based on cumulative input tokens"

### US-3: Context Warning State

**As a** user approaching context limits
**I want to** be warned before hitting the limit
**So that** I can decide whether to continue or start fresh

**Acceptance Criteria:**
- Warning banner appears when estimate exceeds 75%
- Banner shows: "Context window ~75% full. Consider starting a new conversation."
- Banner is dismissible per conversation
- Critical warning at 90%+ with stronger messaging

## Design

### Chat Header Badge

```
┌─────────────────────────────────────────────────────────────┐
│ ConversationToolbar                                         │
│ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌────────────────┐  │
│ │ Model ▼  │ │ Perms ▼  │ │ Tools ▼  │ │ Context: ~23%  │  │
│ └──────────┘ └──────────┘ └──────────┘ └────────────────┘  │
└─────────────────────────────────────────────────────────────┘

Badge states:
  [Context: ~23%]     - Green background
  [Context: ~62%]     - Yellow background
  [⚠ Context: ~78%]  - Orange background, warning icon
  [⚠ Context: ~94%]  - Red background, warning icon
```

### Details Panel Context Section

```
┌─────────────────────────────────────┐
│ Context                             │
├─────────────────────────────────────┤
│ ████████████░░░░░░░░  ~58%         │
│                                     │
│ Input tokens     116,234            │
│ Output tokens     12,456            │
│ Cache read        89,000 (free)     │
│ Cache creation    45,000            │
│ ─────────────────────────           │
│ Est. context     116,234 / 200,000  │
│ Total cost       $0.4523            │
│                                     │
│ ℹ Estimated from cumulative input   │
│   tokens. Actual may vary due to    │
│   SDK context management.           │
└─────────────────────────────────────┘
```

## Technical Implementation

### Data Flow

1. Each assistant message already stores `inputTokens`, `outputTokens` in `ConversationMessage`
2. Need to also extract cache tokens from `claudeMessage.message.usage`
3. Sum tokens across all messages in conversation
4. Calculate percentage based on selected model's context limit
5. Display in both toolbar badge and details section

### Model Context Limits Constant

```typescript
const MODEL_CONTEXT_LIMITS: Record<string, number> = {
  default: 200_000,
  opus: 200_000,
  sonnet: 1_000_000,  // 1M context variant
  haiku: 200_000
}
```

### Components to Modify

| Component | Changes |
|-----------|---------|
| `ConversationToolbar.tsx` | Add ContextBadge component |
| `ConversationDetails.tsx` | Enhance ContextMetricsSection with percentage, progress bar, cache breakdown |
| `chat-store.ts` | Add helper to calculate context metrics from messages |

### New Types

```typescript
interface ContextMetrics {
  inputTokens: number
  outputTokens: number
  cacheReadTokens: number
  cacheCreationTokens: number
  totalCost: number
  contextLimit: number
  estimatedPercentage: number
}
```

## Out of Scope

- Actual context window API (not available in SDK)
- Component-level breakdown (system prompt vs tools vs messages)
- Compaction detection/notification
- Automatic new conversation suggestion

## Success Metrics

- Users can see estimated context usage at a glance
- Clear warnings before approaching limits
- Transparent about estimation limitations
- Cache token visibility for cost optimization

## Known Limitations

1. **Estimation only** - Cannot get actual context window state from SDK
2. **Cumulative vs actual** - SDK may compact context, our estimate doesn't reflect this
3. **No component breakdown** - Can't show system prompt vs tool definitions vs history
4. **Model detection** - Relies on conversation settings, not actual model used if overridden

## Future Improvements

1. Update calculation when Anthropic provides official guidance (SDK issue #66)
2. Detect `SDKCompactBoundaryMessage` to reset/adjust estimates
3. Add "Start new conversation" quick action when context is high
4. Historical context usage trends across conversations
